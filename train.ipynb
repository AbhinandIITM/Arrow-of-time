{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0095cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "from typing import Tuple, Optional\n",
    "import os \n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9182ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TCAM(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Class-Activation Map Network for Arrow of Time detection.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input: Optical flow frames (multiple temporal chunks)\n",
    "    - Processing: Temporal chunking + Parallel VGG-16 backbones\n",
    "    - Fusion: Late temporal fusion of conv5 features\n",
    "    - Classification: 3 conv layers + Global Average Pooling + Logistic Regression\n",
    "    \n",
    "    Args:\n",
    "        num_temporal_chunks (int): Number of temporal chunks (T). Default: 2\n",
    "        frames_per_chunk (int): Frames per temporal chunk. Default: 10\n",
    "        num_flow_channels (int): Input channels (optical flow components). Default: 2 (u, v)\n",
    "        pretrained (bool): Whether to use pretrained VGG16 weights. Default: False\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_temporal_chunks: int = 2,\n",
    "        frames_per_chunk: int = 10,\n",
    "        num_flow_channels: int = 2,\n",
    "        pretrained: bool = False\n",
    "    ):\n",
    "        super(TCAM, self).__init__()\n",
    "        \n",
    "        self.num_temporal_chunks = num_temporal_chunks\n",
    "        self.frames_per_chunk = frames_per_chunk\n",
    "        self.num_flow_channels = num_flow_channels\n",
    "        self.total_frames = num_temporal_chunks * frames_per_chunk\n",
    "        self.total_input_channels = num_flow_channels * frames_per_chunk\n",
    "        \n",
    "        # ==================== Temporal Feature Fusion Stage ====================\n",
    "        # Modified VGG-16 backbone to accept optical flow input\n",
    "        # Expand conv1 filters to accept stacked optical flow frames\n",
    "        \n",
    "        self.features = self._build_vgg16_backbone(pretrained)\n",
    "        \n",
    "        # ==================== Classification Stage ====================\n",
    "        # Replace FC layers with conv layers for better interpretability\n",
    "        \n",
    "        # Three convolutional layers: 3×3×1024 with BatchNorm\n",
    "        # NEW: 512 * num_temporal_chunks\n",
    "        self.class_conv1 = nn.Conv2d(\n",
    "            512 * self.num_temporal_chunks,  # 512*T\n",
    "            1024,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(1024)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.class_conv2 = nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(1024)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.class_conv3 = nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(1024)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Binary logistic regression for forward/backward classification\n",
    "        self.fc = nn.Linear(1024, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def _build_vgg16_backbone(self, pretrained: bool = False) -> nn.Sequential:\n",
    "        \"\"\"\n",
    "        Build VGG16 backbone modified for optical flow input.\n",
    "        \n",
    "        Args:\n",
    "            pretrained (bool): Use pretrained ImageNet weights\n",
    "            \n",
    "        Returns:\n",
    "            nn.Sequential: VGG16 feature extraction layers (up to conv5)\n",
    "        \"\"\"\n",
    "        # Standard VGG16 configuration: conv layer depths for each block\n",
    "        vgg16_config = [\n",
    "            64, 64, 'M',                    # Block 1\n",
    "            128, 128, 'M',                  # Block 2\n",
    "            256, 256, 256, 'M',             # Block 3\n",
    "            512, 512, 512, 'M',             # Block 4\n",
    "            512, 512, 512                   # Block 5 (no final pooling)\n",
    "        ]\n",
    "        \n",
    "        layers = []\n",
    "        in_channels = self.total_input_channels  # Stacked optical flow input\n",
    "        \n",
    "        for v in vgg16_config:\n",
    "            if v == 'M':\n",
    "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            else:\n",
    "                layers.append(nn.Conv2d(in_channels, v, kernel_size=3, padding=1))\n",
    "                layers.append(nn.BatchNorm2d(v))\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "                in_channels = v\n",
    "        \n",
    "        # Load pretrained weights if requested (from torchvision VGG16)\n",
    "        if pretrained:\n",
    "            try:\n",
    "                import torchvision.models as models\n",
    "                vgg16_pretrained = models.vgg16(pretrained=True)\n",
    "                # Copy weights, skipping first conv layer due to channel mismatch\n",
    "                for i, layer in enumerate(layers):\n",
    "                    if isinstance(layer, nn.Conv2d) and i > 0:\n",
    "                        layer.load_state_dict(\n",
    "                            vgg16_pretrained.features[i].state_dict(),\n",
    "                            strict=False\n",
    "                        )\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load pretrained weights: {e}\")\n",
    "                print(\"Proceeding with random initialization\")\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _extract_temporal_chunks(self, x: torch.Tensor) -> list:\n",
    "        \"\"\"\n",
    "        Extract temporal chunks from input batch.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape \n",
    "                (batch_size, total_input_channels, height, width)\n",
    "                where total_input_channels = num_flow_channels * total_frames\n",
    "                \n",
    "        Returns:\n",
    "            list: List of tensors, each of shape (batch_size, chunk_input_channels, height, width)\n",
    "        \"\"\"\n",
    "        batch_size, total_channels, height, width = x.shape\n",
    "        chunk_input_channels = self.num_flow_channels * self.frames_per_chunk\n",
    "        \n",
    "        chunks = []\n",
    "        for chunk_idx in range(self.num_temporal_chunks):\n",
    "            start_channel = chunk_idx * chunk_input_channels\n",
    "            end_channel = start_channel + chunk_input_channels\n",
    "            chunk = x[:, start_channel:end_channel, :, :]\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _temporal_fusion(self, chunk_features: list) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Late temporal fusion: concatenate conv5 features from all chunks.\n",
    "        \n",
    "        Args:\n",
    "            chunk_features (list): List of conv5 feature maps from each chunk,\n",
    "                each of shape (batch_size, 512, H, W)\n",
    "                \n",
    "        Returns:\n",
    "            torch.Tensor: Concatenated features of shape (batch_size, 512*T, H, W)\n",
    "                where T = num_temporal_chunks\n",
    "        \"\"\"\n",
    "        # Concatenate along channel dimension\n",
    "        fused_features = torch.cat(chunk_features, dim=1)  # (B, 512*T, H, W)\n",
    "        return fused_features\n",
    "    \n",
    "    def extract_cam_features(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Extract features for Class Activation Map visualization.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input optical flow tensor\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]:\n",
    "                - conv5_features: Feature maps before classification head (B, 512*T, H, W)\n",
    "                - output: Model output logits (B, 1)\n",
    "        \"\"\"\n",
    "        # Extract temporal chunks\n",
    "        chunks = self._extract_temporal_chunks(x)\n",
    "        \n",
    "        # Process each chunk through VGG16 backbone\n",
    "        chunk_features = []\n",
    "        for chunk in chunks:\n",
    "            # Extract conv5 features by running through backbone\n",
    "            feat = self.features(chunk)\n",
    "            chunk_features.append(feat)\n",
    "        \n",
    "        # Temporal fusion\n",
    "        fused_features = self._temporal_fusion(chunk_features)\n",
    "        \n",
    "        # Save fused features for CAM computation\n",
    "        conv5_features = fused_features\n",
    "        \n",
    "        # Classification head\n",
    "        x = self.class_conv1(fused_features)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.class_conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.class_conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        # Get features before GAP for CAM\n",
    "        pre_gap_features = x  # (B, 1024, H, W)\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        x = self.gap(x)  # (B, 1024, 1, 1)\n",
    "        x = x.view(x.size(0), -1)  # (B, 1024)\n",
    "        \n",
    "        # Classification output\n",
    "        output = self.fc(x)  # (B, 1)\n",
    "        \n",
    "        return {\n",
    "            'conv5_features': conv5_features,\n",
    "            'pre_gap_features': pre_gap_features,\n",
    "            'output': output\n",
    "        }\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of T-CAM model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input optical flow tensor of shape\n",
    "                (batch_size, num_flow_channels * num_temporal_chunks * frames_per_chunk, H, W)\n",
    "                Example: (B, 2*2*10=40, H, W) for default configuration\n",
    "                \n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape (batch_size, 1)\n",
    "                Values close to 1.0 indicate forward-playing video\n",
    "                Values close to 0.0 indicate backward-playing video\n",
    "        \"\"\"\n",
    "        # Extract temporal chunks\n",
    "        chunks = self._extract_temporal_chunks(x)\n",
    "        \n",
    "        # Process each chunk through VGG16 backbone\n",
    "        chunk_features = []\n",
    "        for chunk in chunks:\n",
    "            feat = self.features(chunk)\n",
    "            chunk_features.append(feat)\n",
    "        \n",
    "        # Temporal fusion (late fusion)\n",
    "        fused_features = self._temporal_fusion(chunk_features)  # (B, 512*T, H, W)\n",
    "        \n",
    "        # Classification head\n",
    "        x = self.class_conv1(fused_features)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.class_conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.class_conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        x = self.gap(x)  # (B, 1024, 1, 1)\n",
    "        x = x.view(x.size(0), -1)  # (B, 1024)\n",
    "        \n",
    "        # Binary classification logits\n",
    "        output = self.fc(x)  # (B, 1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_classification_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get classification probabilities (0-1 range).\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input optical flow tensor\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Probabilities of shape (batch_size, 1) in range [0, 1]\n",
    "        \"\"\"\n",
    "        logits = self.forward(x)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        return probs\n",
    "    \n",
    "    def compute_class_activation_map(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        target_class: int = 1,\n",
    "        normalize: bool = True\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute Class Activation Map (CAM) for visualization.\n",
    "        \n",
    "        This method visualizes which spatial regions contribute most to the\n",
    "        forward/backward classification decision.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input optical flow tensor\n",
    "            target_class (int): Target class (0 for backward, 1 for forward). Default: 1\n",
    "            normalize (bool): Normalize CAM to [0, 1]. Default: True\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Class Activation Map of shape (batch_size, 1, H, W)\n",
    "                High values indicate regions important for predicting target_class\n",
    "        \"\"\"\n",
    "        # Extract features and get model output\n",
    "        features_dict = self.extract_cam_features(x)\n",
    "        pre_gap_features = features_dict['pre_gap_features']  # (B, 1024, H, W)\n",
    "        \n",
    "        # Get weights from fc layer for the target class\n",
    "        fc_weights = self.fc.weight[0]  # (1024,)\n",
    "        \n",
    "        # Compute CAM as weighted sum of feature maps\n",
    "        batch_size, num_channels, height, width = pre_gap_features.shape\n",
    "        \n",
    "        # Reshape weights for broadcasting\n",
    "        weights = fc_weights.view(1, num_channels, 1, 1)\n",
    "        \n",
    "        # Compute weighted feature maps\n",
    "        cam = (pre_gap_features * weights).sum(dim=1, keepdim=True)  # (B, 1, H, W)\n",
    "        \n",
    "        # Apply ReLU to focus on positive activations\n",
    "        cam = F.relu(cam)\n",
    "        \n",
    "        # Normalize CAM\n",
    "        if normalize:\n",
    "            # Normalize each sample independently\n",
    "            for i in range(batch_size):\n",
    "                cam_min = cam[i].min()\n",
    "                cam_max = cam[i].max()\n",
    "                if cam_max - cam_min > 0:\n",
    "                    cam[i] = (cam[i] - cam_min) / (cam_max - cam_min)\n",
    "                else:\n",
    "                    cam[i] = torch.zeros_like(cam[i])\n",
    "        \n",
    "        return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4b992fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==================== Utility Functions ====================\n",
    "\n",
    "def create_tcam_model(\n",
    "    num_temporal_chunks: int = 2,\n",
    "    frames_per_chunk: int = 10,\n",
    "    num_flow_channels: int = 2,\n",
    "    pretrained: bool = False,\n",
    "    device: Optional[torch.device] = None\n",
    ") -> TCAM:\n",
    "    \"\"\"\n",
    "    Create a T-CAM model instance.\n",
    "    \n",
    "    Args:\n",
    "        num_temporal_chunks (int): Number of temporal chunks. Default: 2\n",
    "        frames_per_chunk (int): Frames per chunk. Default: 10\n",
    "        num_flow_channels (int): Input flow channels. Default: 2 (u, v)\n",
    "        pretrained (bool): Use pretrained weights. Default: False\n",
    "        device (torch.device): Device to move model to. Default: None\n",
    "        \n",
    "    Returns:\n",
    "        TCAM: Instantiated model\n",
    "    \"\"\"\n",
    "    model = TCAM(\n",
    "        num_temporal_chunks=num_temporal_chunks,\n",
    "        frames_per_chunk=frames_per_chunk,\n",
    "        num_flow_channels=num_flow_channels,\n",
    "        pretrained=pretrained\n",
    "    )\n",
    "    \n",
    "    if device is not None:\n",
    "        model = model.to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    \"\"\"Count total number of trainable parameters.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def print_model_summary(model: TCAM) -> None:\n",
    "    \"\"\"Print model architecture summary.\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"T-CAM Model Summary\")\n",
    "    print(\"=\" * 80)\n",
    "    print(model)\n",
    "    print(f\"\\nTotal Parameters: {count_parameters(model):,}\")\n",
    "    print(f\"Model Size: {count_parameters(model) * 4 / (1024**2):.2f} MB\")\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68825e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "================================================================================\n",
      "T-CAM Model Summary\n",
      "================================================================================\n",
      "TCAM(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(20, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU(inplace=True)\n",
      "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU(inplace=True)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "  )\n",
      "  (class_conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (class_conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (class_conv3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (gap): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "Total Parameters: 43,054,721\n",
      "Model Size: 164.24 MB\n",
      "================================================================================\n",
      "\n",
      "Input shape: torch.Size([4, 40, 224, 224])\n",
      "Output shape: torch.Size([4, 1])\n",
      "Output logits (raw): tensor([0.2983, 0.3716, 0.3421, 0.3473], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Output probabilities: tensor([0.5740, 0.5919, 0.5847, 0.5860], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Predictions (1=Forward, 0=Backward): tensor([1, 1, 1, 1], device='cuda:0')\n",
      "\n",
      "Class Activation Map shape: torch.Size([4, 1, 14, 14])\n",
      "CAM value range: [0.0000, 1.0000]\n",
      "\n",
      "✓ Model created and tested successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "model = create_tcam_model(\n",
    "    num_temporal_chunks=2,\n",
    "    frames_per_chunk=10,\n",
    "    num_flow_channels=2,\n",
    "    pretrained=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "print_model_summary(model)\n",
    "\n",
    "# Example forward pass\n",
    "batch_size = 4\n",
    "height, width = 224, 224\n",
    "num_flow_channels = 2\n",
    "total_frames = 20  # 2 chunks * 10 frames\n",
    "\n",
    "# Create dummy optical flow input\n",
    "# Shape: (batch_size, num_flow_channels * total_frames, height, width)\n",
    "x = torch.randn(\n",
    "    batch_size,\n",
    "    num_flow_channels * total_frames,\n",
    "    height,\n",
    "    width,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output logits (raw): {output.squeeze()}\")\n",
    "\n",
    "# Get probabilities\n",
    "probs = model.get_classification_probabilities(x)\n",
    "print(f\"Output probabilities: {probs.squeeze()}\")\n",
    "print(f\"Predictions (1=Forward, 0=Backward): {(probs > 0.5).long().squeeze()}\")\n",
    "\n",
    "# Compute CAM\n",
    "cam = model.compute_class_activation_map(x, target_class=1)\n",
    "print(f\"\\nClass Activation Map shape: {cam.shape}\")\n",
    "print(f\"CAM value range: [{cam.min():.4f}, {cam.max():.4f}]\")\n",
    "\n",
    "print(\"\\n✓ Model created and tested successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e28a0ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== OPTICAL FLOW EXTRACTION ====================\n",
    "\n",
    "class OpticalFlowExtractor:\n",
    "    \"\"\"\n",
    "    Extract optical flow using TV-L1 method (as used in the paper).\n",
    "    Requires OpenCV with contrib modules.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, backend: str = 'opencv'):\n",
    "        \"\"\"\n",
    "        Initialize optical flow extractor.\n",
    "        \n",
    "        Args:\n",
    "            backend (str): 'opencv' for OpenCV DualTVL1, 'farneback' for Farneback\n",
    "        \"\"\"\n",
    "        self.backend = backend\n",
    "        self._setup_backend()\n",
    "    \n",
    "    def _setup_backend(self):\n",
    "        \"\"\"Setup the optical flow backend.\"\"\"\n",
    "        if self.backend == 'opencv':\n",
    "            try:\n",
    "                self.optical_flow = cv2.optflow.createOptFlow_DualTVL1()\n",
    "            except AttributeError:\n",
    "                print(\"Warning: DualTVL1 not available. Using Farneback instead.\")\n",
    "                print(\"Install opencv-contrib-python: pip install opencv-contrib-python\")\n",
    "                self.backend = 'farneback'\n",
    "        \n",
    "        if self.backend == 'farneback':\n",
    "            self.optical_flow = None  # Use cv2.calcOpticalFlowFarneback\n",
    "    \n",
    "    def extract_flow(self, frame1: np.ndarray, frame2: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extract optical flow between two consecutive frames.\n",
    "        \n",
    "        Args:\n",
    "            frame1 (np.ndarray): Previous frame (grayscale or RGB)\n",
    "            frame2 (np.ndarray): Current frame (grayscale or RGB)\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Optical flow of shape (H, W, 2) containing (u, v) components\n",
    "        \"\"\"\n",
    "        # Convert to grayscale if needed\n",
    "        if len(frame1.shape) == 3:\n",
    "            frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "        if len(frame2.shape) == 3:\n",
    "            frame2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        if self.backend == 'opencv':\n",
    "            flow = self.optical_flow.calc(frame1, frame2, None)\n",
    "        else:  # Farneback\n",
    "            flow = cv2.calcOpticalFlowFarneback(\n",
    "                frame1, frame2,\n",
    "                None,\n",
    "                pyr_scale=0.5,\n",
    "                levels=3,\n",
    "                winsize=15,\n",
    "                iterations=3,\n",
    "                n8=False,\n",
    "                poly_n=5,\n",
    "                poly_sigma=1.2,\n",
    "                flags=0\n",
    "            )\n",
    "        \n",
    "        return flow.astype(np.float32)\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_flow_sequence(\n",
    "        frames: np.ndarray,\n",
    "        method: str = 'consecutive'\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extract optical flow for a sequence of frames.\n",
    "        \n",
    "        Args:\n",
    "            frames (np.ndarray): Array of frames, shape (T, H, W, 3)\n",
    "            method (str): 'consecutive' for frame-to-frame flow\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Optical flow sequence, shape (T-1, H, W, 2)\n",
    "        \"\"\"\n",
    "        extractor = OpticalFlowExtractor(backend='opencv')\n",
    "        flows = []\n",
    "        \n",
    "        for i in range(len(frames) - 1):\n",
    "            flow = extractor.extract_flow(frames[i], frames[i + 1])\n",
    "            flows.append(flow)\n",
    "        \n",
    "        return np.stack(flows, axis=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def visualize_flow(\n",
    "        flow: np.ndarray,\n",
    "        colorize: bool = True\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Visualize optical flow.\n",
    "        \n",
    "        Args:\n",
    "            flow (np.ndarray): Optical flow (H, W, 2)\n",
    "            colorize (bool): Use HSV colorization\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Visualization image (H, W, 3)\n",
    "        \"\"\"\n",
    "        if colorize:\n",
    "            mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "            hsv = np.zeros((flow.shape[0], flow.shape[1], 3), dtype=np.uint8)\n",
    "            hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "            hsv[..., 1] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "            hsv[..., 2] = 255\n",
    "            rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "            return rgb\n",
    "        else:\n",
    "            # Magnitude visualization\n",
    "            mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "            mag = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "            return cv2.applyColorMap(mag, cv2.COLORMAP_JET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "759af29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseFilmFlowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Uses precomputed TV-L1 flow from ./flow_11/stab_10.\n",
    "    labels_* file lines:\n",
    "        \"<Movie>/<Clip> 0\"      -> original order\n",
    "        \"<Movie>/<Clip>_rev 1\"  -> time-reversed\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        flow_root,\n",
    "        label_file,\n",
    "        num_temporal_chunks=2,\n",
    "        frames_per_chunk=10,\n",
    "        image_size=(224, 224),\n",
    "    ):\n",
    "        self.flow_root = flow_root\n",
    "        self.num_temporal_chunks = num_temporal_chunks\n",
    "        self.frames_per_chunk = frames_per_chunk\n",
    "        self.total_frames = num_temporal_chunks * frames_per_chunk\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.samples = []\n",
    "        with open(label_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                rel_path, lab = line.strip().split()\n",
    "                self.samples.append((rel_path, int(lab)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rel_path, label = self.samples[idx]\n",
    "        # handle _rev\n",
    "        is_rev = rel_path.endswith(\"_rev\")\n",
    "        if is_rev:\n",
    "            rel_path = rel_path[:-4]  # strip \"_rev\"\n",
    "\n",
    "        movie, clip = rel_path.split(\"/\")\n",
    "        flow_path = os.path.join(self.flow_root, movie, f\"{clip}.npy\")\n",
    "\n",
    "        flows = np.load(flow_path)  # (T_raw, H0, W0, 2)\n",
    "        if is_rev:\n",
    "            flows = flows[::-1].copy()  # reverse time\n",
    "\n",
    "        T_raw, H0, W0, _ = flows.shape\n",
    "\n",
    "        # ensure exactly total_frames flows\n",
    "        if T_raw >= self.total_frames:\n",
    "            flows = flows[:self.total_frames]\n",
    "        else:\n",
    "            last = flows[-1]\n",
    "            while flows.shape[0] < self.total_frames:\n",
    "                flows = np.concatenate([flows, last[None]], axis=0)\n",
    "\n",
    "        H, W = self.image_size\n",
    "        if (H0, W0) != (H, W):\n",
    "            resized = []\n",
    "            for f in flows:\n",
    "                fx = cv2.resize(f[..., 0], (W, H))\n",
    "                fy = cv2.resize(f[..., 1], (W, H))\n",
    "                resized.append(np.stack([fx, fy], axis=-1))\n",
    "            flows = np.stack(resized, axis=0)\n",
    "\n",
    "        # per-clip normalization\n",
    "        mu, sigma = flows.mean(), flows.std()\n",
    "        flows = (flows - mu) / (sigma + 1e-6)\n",
    "\n",
    "        # (T, H, W, 2) -> (2*T, H, W)\n",
    "        T, H, W, C = flows.shape\n",
    "        flows = flows.transpose(0, 3, 1, 2).reshape(T * C, H, W)\n",
    "\n",
    "        flow_tensor = torch.from_numpy(flows).float()\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return flow_tensor, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc2f3464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for model training.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 4,\n",
    "        num_epochs: int = 50,\n",
    "        learning_rate: float = 0.001,\n",
    "        weight_decay: float = 0.0005,\n",
    "        num_workers: int = 0,\n",
    "        device: str = 'cuda',\n",
    "        checkpoint_dir: str = './checkpoints',\n",
    "        log_interval: int = 100\n",
    "    ):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.num_workers = num_workers\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.log_interval = log_interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3b3c45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"Trainer class for T-CAM model.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: TCAM,\n",
    "        config: TrainingConfig,\n",
    "        criterion: nn.Module = None,\n",
    "        optimizer: optim.Optimizer = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize trainer.\n",
    "        \n",
    "        Args:\n",
    "            model (TCAM): T-CAM model instance\n",
    "            config (TrainingConfig): Training configuration\n",
    "            criterion (nn.Module): Loss function. Default: BCEWithLogitsLoss\n",
    "            optimizer (optim.Optimizer): Optimizer. Default: SGD\n",
    "        \"\"\"\n",
    "        self.model = model.to(config.device)\n",
    "        for p in self.model.features.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.config = config\n",
    "        self.device = config.device\n",
    "        \n",
    "        # Loss function: Binary classification with logits\n",
    "        if criterion is None:\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "        self.criterion = criterion\n",
    "        \n",
    "        # Optimizer: SGD with momentum as in paper\n",
    "        if optimizer is None:\n",
    "            optimizer = optim.SGD(\n",
    "                model.parameters(),\n",
    "                lr=config.learning_rate,\n",
    "                momentum=0.9,\n",
    "                weight_decay=config.weight_decay\n",
    "            )\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.train_losses = []\n",
    "        self.val_accuracies = []\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    \n",
    "    def train_epoch(self, train_loader: DataLoader) -> float:\n",
    "        \"\"\"\n",
    "        Train for one epoch.\n",
    "        \n",
    "        Args:\n",
    "            train_loader (DataLoader): Training data loader\n",
    "            \n",
    "        Returns:\n",
    "            float: Average training loss\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (flows, labels) in enumerate(tqdm(train_loader, desc=\"Train\", leave=False)):\n",
    "            flows = flows.to(self.device)\n",
    "            labels = labels.to(self.device).float().unsqueeze(1)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(flows)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Logging\n",
    "            if (batch_idx + 1) % self.config.log_interval == 0:\n",
    "                print(f\"Batch [{batch_idx + 1}/{len(train_loader)}], \"\n",
    "                      f\"Loss: {loss.item():.4f}\")\n",
    "            \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        self.train_losses.append(avg_loss)\n",
    "        return avg_loss\n",
    "    \n",
    "    def validate(self, val_loader: DataLoader) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Validate model.\n",
    "        \n",
    "        Args:\n",
    "            val_loader (DataLoader): Validation data loader\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[float, float]: (Accuracy, Loss)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for flows, labels in tqdm(val_loader, desc=\"Val\", leave=False):\n",
    "                flows = flows.to(self.device)\n",
    "                labels = labels.to(self.device).float().unsqueeze(1)\n",
    "                \n",
    "                outputs = self.model(flows)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Compute accuracy\n",
    "                predictions = (torch.sigmoid(outputs) > 0.5).long()\n",
    "                correct += (predictions == labels.long()).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        self.val_accuracies.append(accuracy)\n",
    "        \n",
    "        return accuracy, avg_loss\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: Optional[DataLoader] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Full training loop.\n",
    "        \n",
    "        Args:\n",
    "            train_loader (DataLoader): Training data loader\n",
    "            val_loader (Optional[DataLoader]): Validation data loader\n",
    "        \"\"\"\n",
    "        print(f\"Starting training for {self.config.num_epochs} epochs...\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"\\n--- Epoch {epoch + 1}/{self.config.num_epochs} ---\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            print(f\"Average Training Loss: {train_loss:.4f}\")\n",
    "            \n",
    "            # Validate\n",
    "            if val_loader is not None:\n",
    "                val_acc, val_loss = self.validate(val_loader)\n",
    "                print(f\"Validation Accuracy: {val_acc:.4f}, Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            self._save_checkpoint(epoch)\n",
    "            self.scheduler.step()\n",
    "\n",
    "    \n",
    "    def _save_checkpoint(self, epoch: int):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        import os\n",
    "        os.makedirs(self.config.checkpoint_dir, exist_ok=True)\n",
    "        checkpoint_path = os.path.join(\n",
    "            self.config.checkpoint_dir,\n",
    "            f\"tcam_epoch_{epoch + 1}.pth\"\n",
    "        )\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_accuracies': self.val_accuracies\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path: str):\n",
    "        \"\"\"Load model from checkpoint.\"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.train_losses = checkpoint.get('train_losses', [])\n",
    "        self.val_accuracies = checkpoint.get('val_accuracies', [])\n",
    "        print(f\"Checkpoint loaded from: {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf051f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================== INFERENCE UTILITIES ====================\n",
    "\n",
    "class Inference:\n",
    "    \"\"\"Inference utilities for arrow of time detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: TCAM, device: torch.device):\n",
    "        \"\"\"\n",
    "        Initialize inference engine.\n",
    "        \n",
    "        Args:\n",
    "            model (TCAM): Trained model\n",
    "            device (torch.device): Device to run inference on\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "    \n",
    "    def predict(self, flows: torch.Tensor) -> Tuple[float, str]:\n",
    "        \"\"\"\n",
    "        Make prediction on optical flow.\n",
    "        \n",
    "        Args:\n",
    "            flows (torch.Tensor): Optical flow tensor\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[float, str]: (Probability, Direction) where Direction is 'Forward' or 'Backward'\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            flows = flows.to(self.device)\n",
    "            if flows.dim() == 3:\n",
    "                flows = flows.unsqueeze(0)\n",
    "            \n",
    "            probs = self.model.get_classification_probabilities(flows)\n",
    "            prob = probs.item()\n",
    "            direction = 'Forward' if prob > 0.5 else 'Backward'\n",
    "        \n",
    "        return prob, direction\n",
    "    \n",
    "    def predict_with_cam(self, flows: torch.Tensor) -> Tuple[float, str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Make prediction and compute Class Activation Map.\n",
    "        \n",
    "        Args:\n",
    "            flows (torch.Tensor): Optical flow tensor\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[float, str, np.ndarray]: (Probability, Direction, CAM)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            flows = flows.to(self.device)\n",
    "            if flows.dim() == 3:\n",
    "                flows = flows.unsqueeze(0)\n",
    "            \n",
    "            # Get prediction\n",
    "            probs = self.model.get_classification_probabilities(flows)\n",
    "            prob = probs.item()\n",
    "            direction = 'Forward' if prob > 0.5 else 'Backward'\n",
    "            \n",
    "            # Get CAM\n",
    "            cam = self.model.compute_class_activation_map(flows, target_class=1)\n",
    "            cam = cam[0, 0].cpu().numpy()  # (H, W)\n",
    "        \n",
    "        return prob, direction, cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5decfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-CAM Training Utilities\n",
      "================================================================================\n",
      "\n",
      "Trainer initialized successfully!\n",
      "Device: cuda\n",
      "Batch size: 4\n",
      "Learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"T-CAM Training Utilities\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configuration\n",
    "config = TrainingConfig(\n",
    "    batch_size=4,\n",
    "    num_epochs=30,\n",
    "    learning_rate=1e-4,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = create_tcam_model(device=config.device)\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(model, config)\n",
    "\n",
    "print(\"\\nTrainer initialized successfully!\")\n",
    "print(f\"Device: {config.device}\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "print(f\"Learning rate: {config.learning_rate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c29206ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/val split...\n",
      "Train labels set: {0, 1}\n",
      "Val labels set: {0, 1}\n",
      "Starting training for 30 epochs...\n",
      "Device: cuda\n",
      "\n",
      "--- Epoch 1/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6950\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_1.pth\n",
      "\n",
      "--- Epoch 2/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6851\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_2.pth\n",
      "\n",
      "--- Epoch 3/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6872\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_3.pth\n",
      "\n",
      "--- Epoch 4/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5833, Loss: 0.6835\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_4.pth\n",
      "\n",
      "--- Epoch 5/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4167, Loss: 0.6900\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_5.pth\n",
      "\n",
      "--- Epoch 6/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4167, Loss: 0.6865\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_6.pth\n",
      "\n",
      "--- Epoch 7/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6862\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_7.pth\n",
      "\n",
      "--- Epoch 8/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6867\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_8.pth\n",
      "\n",
      "--- Epoch 9/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6854\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_9.pth\n",
      "\n",
      "--- Epoch 10/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4167, Loss: 0.6914\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_10.pth\n",
      "\n",
      "--- Epoch 11/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6910\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_11.pth\n",
      "\n",
      "--- Epoch 12/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6888\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_12.pth\n",
      "\n",
      "--- Epoch 13/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6912\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_13.pth\n",
      "\n",
      "--- Epoch 14/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5833, Loss: 0.6957\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_14.pth\n",
      "\n",
      "--- Epoch 15/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6930\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_15.pth\n",
      "\n",
      "--- Epoch 16/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6911\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_16.pth\n",
      "\n",
      "--- Epoch 17/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6914\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_17.pth\n",
      "\n",
      "--- Epoch 18/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6932\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_18.pth\n",
      "\n",
      "--- Epoch 19/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6928\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_19.pth\n",
      "\n",
      "--- Epoch 20/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6945\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_20.pth\n",
      "\n",
      "--- Epoch 21/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6908\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_21.pth\n",
      "\n",
      "--- Epoch 22/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6943\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_22.pth\n",
      "\n",
      "--- Epoch 23/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5833, Loss: 0.6890\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_23.pth\n",
      "\n",
      "--- Epoch 24/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6956\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_24.pth\n",
      "\n",
      "--- Epoch 25/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6924\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_25.pth\n",
      "\n",
      "--- Epoch 26/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6935\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_26.pth\n",
      "\n",
      "--- Epoch 27/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6915\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_27.pth\n",
      "\n",
      "--- Epoch 28/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6918\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_28.pth\n",
      "\n",
      "--- Epoch 29/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6950\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_29.pth\n",
      "\n",
      "--- Epoch 30/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, Loss: 0.6902\n",
      "Checkpoint saved: ./checkpoints\\tcam_epoch_30.pth\n"
     ]
    }
   ],
   "source": [
    "# flow_root = \"./flow_11/stab_10\"\n",
    "# labels = \"./labels_bidirectional.txt\"\n",
    "flow_root = os.path.join(os.getcwd(),'flow_11','stab_10')\n",
    "labels = os.path.join(os.getcwd(),'labels_bidirectional.txt')\n",
    "full_dataset = ReverseFilmFlowDataset(\n",
    "    flow_root=flow_root,\n",
    "    label_file=labels,\n",
    "    num_temporal_chunks=2,\n",
    "    frames_per_chunk=10,\n",
    "    image_size=(224, 224),\n",
    ")\n",
    "\n",
    "print(\"Train/val split...\")\n",
    "from collections import defaultdict\n",
    "movies = defaultdict(list)\n",
    "for idx, (rel_path, lab) in enumerate(full_dataset.samples):\n",
    "    movie = rel_path.split(\"/\")[0].replace(\"_rev\", \"\")\n",
    "    movies[movie].append(idx)\n",
    "\n",
    "movie_names = sorted(movies.keys())\n",
    "n_train_movies = int(0.8 * len(movie_names))\n",
    "train_movies = set(movie_names[:n_train_movies])\n",
    "val_movies = set(movie_names[n_train_movies:])\n",
    "\n",
    "train_indices = [i for m in train_movies for i in movies[m]]\n",
    "val_indices   = [i for m in val_movies  for i in movies[m]]\n",
    "\n",
    "train_ds = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "val_ds   = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "\n",
    "# sanity check\n",
    "train_labels = [full_dataset.samples[i][1] for i in train_ds.indices]\n",
    "val_labels = [full_dataset.samples[i][1] for i in val_ds.indices]\n",
    "print(\"Train labels set:\", set(train_labels))\n",
    "print(\"Val labels set:\", set(val_labels))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train(train_loader, val_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
